{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1939d6-9dae-45cd-98a8-5b28e76042a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e418b4ba-e7ba-46ab-ae1c-bd4e2a6e19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1:\n",
    "'''\n",
    "• Missing values in a dataset refer to the absence of a value or a piece of information in a particular column or row of the dataset. \n",
    "  This can happen due to various reasons, such as human error, technical issues, or simply because the data does not exist.\n",
    "   \n",
    "• Missing values can be problematic because they can skew the results of statistical analyses, machine learning models, and other data-driven tasks. \n",
    "  Handling missing values properly can improve the accuracy and reliability of the analysis.   \n",
    "'''\n",
    "'''\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "1. Decision trees: Decision trees can handle missing values by considering only the available features when making a split.\n",
    "\n",
    "2. Random forests: Random forests are an extension of decision trees and can also handle missing values in a similar way.\n",
    "\n",
    "3. K-nearest neighbors (KNN): KNN can handle missing values by imputing missing values with the mean or median of the available values.\n",
    "\n",
    "4. Support vector machines (SVM): SVM can handle missing values by treating them as outliers and minimizing their impact on the decision boundary.\n",
    "\n",
    "5. Naive Bayes: Naive Bayes can handle missing values by ignoring them and calculating the probability of the target variable based on the available features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26836239-f2ed-4179-be2c-1ac8db562de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2:\n",
    "'''\n",
    "Following techniques used to handle missing data:\n",
    "\n",
    "• Deletion: In this technique, the missing values are removed from the dataset. \n",
    "\n",
    "It can be further divided into two categories:\n",
    "\n",
    ">> Listwise deletion: In this technique, any row that has missing values is completely removed from the dataset.\n",
    "\n",
    ">> Pairwise deletion: In this technique, only the missing values of a specific feature are removed from the analysis. It is also known as available case analysis.\n",
    "'''\n",
    "\n",
    "'''\n",
    "• Imputation: In this technique, the missing values are replaced with estimated values. \n",
    "              \n",
    "It can be further divided into two categories:\n",
    "\n",
    ">> Mean imputation: In this technique, the missing values are replaced with the mean value of the non-missing values in the same feature.\n",
    "\n",
    ">> Mode imputation: In this technique, the missing values are replaced with the mode value of the non-missing values in the same feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e88299-4397-4888-a8f0-2d023895685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Listwise deletion python code:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0161c95-fff8-488c-96a6-853c562c9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A\n",
      "0  1.0\n",
      "1  2.0\n",
      "3  4.0\n"
     ]
    }
   ],
   "source": [
    "# Pairwise deletion python code:\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n",
    "\n",
    "# drop missing values from column A only\n",
    "df = df[['A']].dropna()\n",
    "\n",
    "# print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa07f28-3293-4eea-9e8f-425a670e8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B\n",
      "0  1.000000  5.0\n",
      "1  2.000000  6.5\n",
      "2  2.333333  6.5\n",
      "3  4.000000  8.0\n"
     ]
    }
   ],
   "source": [
    "# Mean imputation python code:\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n",
    "\n",
    "# impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['A', 'B']] = imputer.fit_transform(df[['A', 'B']])\n",
    "\n",
    "# print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8f440b-b706-4f09-9256-52211fe6fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Mode imputation python code:\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n",
    "\n",
    "# impute missing values with mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['A', 'B']] = imputer.fit_transform(df[['A', 'B']])\n",
    "\n",
    "# print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab81550-ace7-4f97-8b75-e6774d3ce5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3:\n",
    "'''\n",
    "• Imbalanced data refers to a situation in which the distribution of classes in the dataset is unequal. This means that one class has significantly more or fewer samples than the other class(es).\n",
    "\n",
    "• In a binary classification problem, a balanced dataset would have a 50:50 distribution of the two classes, while an imbalanced dataset may have a distribution of, for example, 90:10 or 95:5.\n",
    "\n",
    "'''\n",
    "'''\n",
    "• If imbalanced data is not handled properly, it can lead to biased model predictions and poor performance. \n",
    "\n",
    "## Some of the consequences of not handling imbalanced data are:\n",
    "\n",
    "1. Overfitting: When a model is trained on imbalanced data, it may learn to classify all instances as the majority class, leading to overfitting.\n",
    "\n",
    "2. Biased evaluation: When evaluating the model on imbalanced data, accuracy may not be a good metric to use, as it can be misleading. \n",
    "                      The model may appear to perform well if it predicts the majority class correctly but may perform poorly on the minority class.\n",
    "\n",
    "3. Poor generalization: If the model is trained on imbalanced data, it may not generalize well to new data with a different distribution of classes.\n",
    "\n",
    "4. Unfairness: If the model is used in decision-making scenarios, such as hiring or lending, it may lead to biased and unfair outcomes for the minority class.\n",
    "\n",
    "To overcome these issues, it is essential to handle imbalanced data in a way that allows the model to learn from the minority class and not only focus on the majority class. \n",
    "Some techniques to handle imbalanced data include oversampling the minority class, undersampling the majority class, and using algorithms specifically designed for imbalanced data \n",
    "such as SMOTE (Synthetic Minority Over-sampling Technique) \n",
    "    and ADASYN (Adaptive Synthetic Sampling).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e97c304-ef77-4692-9b2e-16cb8838e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4:\n",
    "'''\n",
    "# Up-sampling and Down-sampling are techniques used to handle imbalanced data.\n",
    "\n",
    "• Up-sampling involves creating more samples for the minority class to balance the distribution of classes. This can be done by either replicating existing samples or generating new synthetic samples.\n",
    "\n",
    "• Down-sampling involves removing some samples from the majority class to balance the distribution of classes. This can be done by either randomly selecting samples or selecting samples based on a specific criterion.\n",
    "'''\n",
    "'''\n",
    "Example when up-sampling and down-sampling are required.\n",
    "\n",
    "• Suppose we have a binary classification problem where we want to predict whether a customer will churn or not. The dataset contains 1000 samples, out of which only 100 (10%) belong to the churn class, while the remaining 900 (90%) belong to the non-churn class.\n",
    "\n",
    "• In this case, the dataset is imbalanced as the distribution of classes is highly skewed towards the non-churn class. If we train a model on this imbalanced dataset without any modifications, it is likely to predict the majority class (non-churn) for most of the test data, leading to poor performance on the minority class (churn).\n",
    "\n",
    ">>> To overcome this issue, we can use either up-sampling or down-sampling.\n",
    "\n",
    "• Up-sampling: In this approach, we can create more samples for the minority class to balance the distribution of classes. For example, we can use the SMOTE technique to generate synthetic samples for the minority class. After up-sampling, the dataset may contain 1000 samples (500 from each class), and we can train the model on this balanced dataset.\n",
    "\n",
    "• Down-sampling: In this approach, we can randomly remove some samples from the majority class to balance the distribution of classes. For example, we can randomly select 100 samples from the non-churn class, and after down-sampling, the dataset may contain 200 samples (100 from each class), and we can train the model on this balanced dataset.\n",
    "\n",
    "• Both up-sampling and down-sampling have their advantages and disadvantages. Up-sampling can lead to overfitting if not done carefully, while down-sampling can result in loss of information from the majority class. The choice of the sampling method depends on the specific problem and the characteristics of the dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeeac635-1754-42b0-a251-25b8d5971422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5:\n",
    "'''\n",
    "Data augmentation:\n",
    "\n",
    "• Data augmentation is a technique used to increase the size of the training dataset by creating new samples from the existing ones.\n",
    "\n",
    "• The idea behind data augmentation is to create variations of the original data that are still representative of the underlying distribution.\n",
    "\n",
    "• Data augmentation can be applied to different types of data, such as images, text, and time-series.\n",
    "\n",
    "• The most common types of data augmentation include image flipping, rotation, and cropping, text swapping and shuffling, and time-series shifting and scaling.\n",
    "\n",
    "SMOTE:\n",
    "\n",
    "• SMOTE (Synthetic Minority Over-sampling Technique) is a technique used to up-sample the minority class in an imbalanced dataset by generating synthetic samples.\n",
    "\n",
    "• The idea behind SMOTE is to create synthetic samples by interpolating between existing samples from the minority class.\n",
    "\n",
    "• To generate a synthetic sample, SMOTE selects a random sample from the minority class and its k nearest neighbors. \n",
    "  It then selects one of the k neighbors randomly and calculates the difference between the feature values of the two samples. \n",
    "  Finally, it multiplies this difference by a random number between 0 and 1 and adds it to the feature values of the selected sample to create a new synthetic sample.\n",
    "\n",
    "• By repeating this process for multiple samples, SMOTE generates a new set of samples that are representative of the minority class and can be used to balance the distribution of classes.\n",
    "\n",
    "• SMOTE is a popular technique in machine learning and has been shown to improve the performance of classifiers on imbalanced datasets.\n",
    "  However, it should be used with caution as it can lead to overfitting if the synthetic samples are too similar to the original ones.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2c98e7-c049-460d-8b2b-14721f19f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6:\n",
    "'''\n",
    "# Outliers in a dataset:\n",
    "\n",
    "• Outliers are data points that are significantly different from the majority of the data points in a dataset.\n",
    "\n",
    "• Outliers can be caused by measurement errors, data processing errors, or genuine anomalies in the data.\n",
    "\n",
    "• Outliers can have a significant impact on the statistical analysis and machine learning models trained on the dataset, as they can distort the overall distribution of the data and lead to biased estimates.\n",
    "\n",
    ">>> Neccessity to handle outliers:\n",
    "\n",
    "• Handling outliers is essential because they can significantly impact the results of data analysis and machine learning models.\n",
    "\n",
    "• Outliers can distort the distribution of the data, making it difficult to estimate the central tendency and variability of the data accurately.\n",
    "\n",
    "• Outliers can also affect the performance of machine learning models by biasing the parameter estimates and reducing the generalization ability of the models.\n",
    "\n",
    "• Therefore, it is important to identify and handle outliers in a dataset before performing any statistical analysis or machine learning tasks.\n",
    "\n",
    "• The specific method used to handle outliers depends on the nature of the data and the analysis or modeling task at hand. \n",
    "  Some common methods for handling outliers include removing them from the dataset, transforming the data, or using robust statistical methods that are less sensitive to outliers.\n",
    "  The choice of method depends on the specific problem and the characteristics of the data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98293b5b-8bf0-4b92-9aa1-b4811d7e7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7:\n",
    "'''\n",
    "Some techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "1. Deletion: This technique involves removing the observations or variables with missing data from the dataset. There are two types of deletion: listwise deletion and pairwise deletion. Listwise deletion removes all observations with missing data, while pairwise deletion removes only the missing values for each variable in the analysis.\n",
    "\n",
    "2. Imputation: This technique involves estimating missing values based on the available data. Imputation methods can be categorized as simple imputation methods or advanced imputation methods. Simple imputation methods include mean imputation, median imputation, and mode imputation, while advanced imputation methods include regression imputation, k-nearest neighbor imputation, and multiple imputation.\n",
    "\n",
    "3. Prediction models: This technique involves using machine learning models to predict the missing values. The models are trained on the available data to predict the missing values for each variable. The predicted values are then used to fill in the missing data.\n",
    "\n",
    "4. Domain knowledge: This technique involves using domain knowledge to estimate the missing values. For example, if the missing data is related to age, gender, or income, demographic data or historical data can be used to estimate the missing values.\n",
    "\n",
    "The choice of technique depends on the amount of missing data, the nature of the data, and the analysis or modeling task at hand. \n",
    "It is important to carefully consider the advantages and disadvantages of each technique before deciding on the best approach for handling missing data in customer data analysis.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c9517c-14b4-47a7-b2dd-ad6673885dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8:\n",
    "'''\n",
    "Some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. Visual inspection: One way to determine if the missing data is missing at random or if there is a pattern is to create visualizations of the data. \n",
    "                      Visual inspection can reveal patterns in the missing data that may not be apparent from summary statistics.\n",
    "\n",
    "2. Summary statistics: Another way to determine if the missing data is missing at random or if there is a pattern is to compute summary statistics for the available data and compare them to the summary statistics for the missing data.\n",
    "\n",
    "3. Correlation analysis: Correlation analysis can be used to determine if there is a relationship between the missing data and other variables in the dataset. \n",
    "                         If there is a correlation, it suggests that the missing data is not missing at random.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becc1bf7-a3c3-4042-acb8-cd32de71eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9:\n",
    "'''\n",
    "Some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset for a medical diagnosis project:\n",
    "\n",
    "1. Confusion matrix: A confusion matrix can provide a detailed breakdown of the model's predictions, including true positive, false positive, true negative, and false negative values. \n",
    "                     From the confusion matrix, metrics such as sensitivity, specificity, precision, and recall can be calculated.\n",
    "\n",
    "2. Receiver Operating Characteristic (ROC) curve: An ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at different classification thresholds. \n",
    "                                                  The area under the curve (AUC) can be used as a metric to evaluate the model's performance.\n",
    "\n",
    "3. Precision-Recall (PR) curve: A PR curve plots precision against recall at different classification thresholds. \n",
    "                                The area under the curve (AUC) can be used as a metric to evaluate the model's performance.\n",
    "\n",
    "4. F1 score: The F1 score is a single metric that combines precision and recall. \n",
    "             It is particularly useful when the dataset is imbalanced, as it gives equal weight to precision and recall.\n",
    "\n",
    "4. Stratified cross-validation: Stratified cross-validation can be used to ensure that each fold of the cross-validation process contains a representative sample of the minority class.\n",
    "\n",
    "5. Resampling techniques: Resampling techniques such as oversampling or undersampling can be used to balance the dataset before training the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f649cdb7-0163-49c8-b0a0-2e9947f7271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10:\n",
    "'''\n",
    "Some methods that can be used to balance an imbalanced dataset and down-sample the majority class:\n",
    "\n",
    "1. Random under-sampling: This involves randomly selecting a subset of the majority class to reduce its size to match the size of the minority class. This method is simple and easy to implement, but may result in a loss of information.\n",
    "\n",
    "2. Cluster centroids: This method involves identifying centroids of the majority class using clustering algorithms and then down-sampling the majority class by randomly selecting samples that are closest to the identified centroids. This method can be more effective than random under-sampling but may be computationally expensive.\n",
    "\n",
    "3. Tomek links: Tomek links are pairs of samples, one from the minority class and one from the majority class, that are closest to each other. Removing the majority class samples from these pairs can result in a down-sampled majority class that is better separated from the minority class.\n",
    "\n",
    "4. Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic samples of the minority class by interpolating between existing samples. This method can be effective in increasing the size of the minority class, but may also result in overfitting.\n",
    "\n",
    "It is important to carefully evaluate the performance of any method used to balance an imbalanced dataset, as downsampling the majority class may result in a loss of information and upsampling the minority class may result in overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f05bce-2732-41d0-8920-841702efa21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 11:\n",
    "'''\n",
    "Some methods that can be used to balance an imbalanced dataset and up-sample the minority class:\n",
    "\n",
    "1. Random over-sampling: This involves randomly duplicating samples from the minority class to increase its size to match the size of the majority class. This method is simple and easy to implement, but may result in overfitting.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique): This method involves creating synthetic samples of the minority class by interpolating between existing samples. This method can be effective in increasing the size of the minority class, but may also result in overfitting.\n",
    "\n",
    "3. ADASYN (Adaptive Synthetic Sampling): This method is an extension of SMOTE that generates more synthetic samples for the minority class that are harder to learn by the classifier.\n",
    "\n",
    "4. Minority class boosting: This method involves giving more weight to samples from the minority class during training of the classifier. This method can be effective in increasing the importance of the minority class in the classification task.\n",
    "\n",
    "It is important to carefully evaluate the performance of any method used to balance an imbalanced dataset, as upsampling the minority class may result in overfitting and can lead to reduced model performance on the test set.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
